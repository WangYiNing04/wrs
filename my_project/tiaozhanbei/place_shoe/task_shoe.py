#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time : 2025/10/27
# @Author : ZhangXi

import os
import pickle
import time
import cv2
import numpy as np
from sklearn.cluster import DBSCAN
from wrs import wd, rm, mgm, mcm, ppp, gg, gpa
import wrs.basis.robot_math as rm
from wrs.vision.depth_camera.util_functions import registration_ptpt
import wrs.modeling.geometric_model as gm
from wrs.robot_sim.robots.piper.piper_dual_arm import DualPiperNoBody
from wrs.robot_con.piper.piper import PiperArmController
import wrs.robot_sim.end_effectors.grippers.piper_gripper.piper_gripper as pg
from my_project.tiaozhanbei.yolo_detect.yolo_utils import init_yolo, init_camera, transform_points_by_homomat, yolo_detect_world_positions
from my_project.tiaozhanbei.place_shoe.constant import YOLO_MODEL_SHOES_PATH, SHOE_MODEL_PATH, GRASP_PATH_SHOES, \
    MIDDLE_CAM_C2W


def align_z_to_up(R):
    """
    Adjusts a rotation matrix so that its z-axis aligns with (0, 0, 1),
    while preserving x/y directions as much as possible.
    """
    R = np.array(R, dtype=np.float64)
    assert R.shape == (3, 3)

    # Force the z-axis to (0, 0, 1)
    z_new = np.array([0, 0, 1], dtype=np.float64)

    # Project original x-axis onto the plane orthogonal to new z
    x_old = R[:, 0]
    x_new = x_old - np.dot(x_old, z_new) * z_new
    x_new /= np.linalg.norm(x_new) + 1e-9

    # Recompute y-axis using cross product to ensure orthogonality
    y_new = np.cross(z_new, x_new)
    y_new /= np.linalg.norm(y_new) + 1e-9

    R_new = np.column_stack((x_new, y_new, z_new))
    return R_new

class MultiCameraShoeTask:
    def __init__(self):
        # ===== ç¡¬ä»¶ä¸æ¨¡å‹ =====
        self.left_arm = PiperArmController(can_name='can0', has_gripper=True)
        self.right_arm = PiperArmController(can_name='can1', has_gripper=True)
        self.rbt_s = DualPiperNoBody()
        self.gripper = pg.PiperGripper()
        self.yolo = init_yolo(YOLO_MODEL_SHOES_PATH)

        # æ‘„åƒå¤´å®šä¹‰
        self.cameras = {
            "middle": {"cam": init_camera(camera_id='middle'), "type": "fixed", "c2w": MIDDLE_CAM_C2W},
            # "left": {"cam": init_camera(camera_id='243322074546'), "type": "handeye", "handeye": LEFT_HAND_EYE},
            # "right": {"cam": init_camera(camera_id='243322071033'), "type": "handeye", "handeye": RIGHT_HAND_EYE}
        }

    def detect_shoes(self, show=False, eps=0.03):
        """ä½¿ç”¨å¤šæ‘„åƒå¤´å’ŒYOLOæ£€æµ‹é‹å­(å¸¦ç‚¹äº‘)å’Œå«å­(ä»…ä¸­å¿ƒç‚¹)"""
        all_results = []

        for name, cam_info in self.cameras.items():
            cam = cam_info["cam"]
            try:
                depth_frame, color_frame = cam._current_frames()
                color_img = np.asanyarray(color_frame.get_data())
                detections = self.yolo(color_img, verbose=False)[0]

                if detections is None or len(detections.boxes) == 0:
                    continue

                for (x1, y1, x2, y2), cls_id, conf in zip(
                        detections.boxes.xyxy.cpu().numpy(),
                        detections.boxes.cls.cpu().numpy(),
                        detections.boxes.conf.cpu().numpy()):
                    if conf < 0.3:
                        continue

                    # æå–è¯¥æ£€æµ‹æ¡†ä¸­çš„ç‚¹äº‘
                    points_3d = cam.points_in_color_bbox((x1, y1, x2, y2))
                    if len(points_3d) == 0:
                        continue

                    # è½¬æ¢åˆ°ä¸–ç•Œåæ ‡
                    if cam_info["type"] == "fixed":
                        points_world = transform_points_by_homomat(cam_info["c2w"], points_3d)
                    else:
                        points_world = self.rbt_s.transform_point_cloud_handeye(
                            cam_info["handeye"], points_3d,
                            component_name='lft_arm' if name == 'left' else 'rgt_arm'
                        )

                    centroid = points_world.mean(axis=0)
                    # ä¿å­˜ (ç±»åˆ«, è´¨å¿ƒ, ç‚¹äº‘)
                    all_results.append((int(cls_id), centroid.tolist(), points_world))

                if show:
                    cv2.imshow(f"{name}_camera", color_img)
                    cv2.waitKey(1)

            except Exception as e:
                print(f"âš ï¸ {name} æ‘„åƒå¤´æ£€æµ‹å¤±è´¥: {e}")
                continue

        if show:
            cv2.destroyAllWindows()

        if not all_results:
            return []

        # DBSCAN å»é‡ï¼ˆåˆå¹¶å¤šç›¸æœºç»“æœï¼‰
        deduped_results = []
        all_cls_ids = [r[0] for r in all_results]
        all_positions = np.array([r[1] for r in all_results])
        unique_cls = set(all_cls_ids)

        for cls in unique_cls:
            mask = [i for i, c in enumerate(all_cls_ids) if c == cls]
            cls_positions = all_positions[mask]
            clustering = DBSCAN(eps=eps, min_samples=1).fit(cls_positions)
            labels = clustering.labels_
            for lbl in np.unique(labels):
                cluster_points = cls_positions[labels == lbl]
                centroid = cluster_points.mean(axis=0)
                if cls == 0:
                    # é‹å­ â†’ è¿”å›ç‚¹äº‘ç”¨äºICPTrue
                    deduped_results.append((cls, centroid.tolist(), all_results[mask[0]][2]))
                else:
                    # å«å­ â†’ åªè¿”å›è´¨å¿ƒï¼Œæ— éœ€ICP
                    deduped_results.append((cls, centroid.tolist(), None))

        return deduped_results

    
    def process_gripper_data(self, data, threshold=0.1029):
        """
        å¤„ç†å¤¹çˆªæ•°æ®ï¼Œå°†å¤§çš„æ•°æ”¹ä¸º1ï¼Œå°çš„æ•°æ”¹ä¸º0ï¼Œå¹¶è¿”å›å˜åŒ–ç‚¹ç´¢å¼•
        
        å‚æ•°:
            data: è¾“å…¥æ•°ç»„ï¼ŒåŒ…å«å¤¹çˆªå®½åº¦æ•°æ®
            threshold: åˆ¤æ–­å¤§å°çš„é˜ˆå€¼ï¼Œé»˜è®¤0.05
            
        è¿”å›:
            tuple: (å¤„ç†åçš„äºŒè¿›åˆ¶æ•°ç»„, å˜åŒ–ç‚¹ç´¢å¼•åˆ—è¡¨)
        """
        # å°†æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
        arr = np.array(data)
        
        # åˆ›å»ºäºŒè¿›åˆ¶æ•°ç»„ï¼šå¤§äºé˜ˆå€¼è®¾ä¸º1ï¼Œå¦åˆ™è®¾ä¸º0
        binary_arr = (arr > threshold).astype(int)
        
        # æ‰¾åˆ°æ•°å€¼å˜åŒ–çš„ç´¢å¼•
        change_indices = np.where(np.diff(binary_arr) != 0)[0] + 1
        
        return binary_arr.tolist(), change_indices.tolist()
    
    def split_trajectory_by_gripper(self, jv, change_indices, threshold=0.05):
        """
        æ ¹æ®å¤¹çˆªæ•°æ®çš„å˜åŒ–ç‚¹å°†å…³èŠ‚é€Ÿåº¦è½¨è¿¹åˆ†å‰²ä¸ºä¸‰æ®µ
        
        å‚æ•°:
            jv: å…³èŠ‚é€Ÿåº¦æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n,6)çš„äºŒç»´æ•°ç»„
            gripper_data: å¤¹çˆªå®½åº¦æ•°æ®ï¼Œä¸€ç»´æ•°ç»„
            threshold: åˆ¤æ–­å¤¹çˆªå¼€åˆçš„é˜ˆå€¼
            
        è¿”å›:
            dict: åŒ…å«ä¸‰æ®µè½¨è¿¹çš„å­—å…¸ {
                'stage1': ç¬¬ä¸€é˜¶æ®µè½¨è¿¹,
                'stage2': ç¬¬äºŒé˜¶æ®µè½¨è¿¹,
                'stage3': ç¬¬ä¸‰é˜¶æ®µè½¨è¿¹,
                'change_points': å˜åŒ–ç‚¹ç´¢å¼•
            }
        """
        # ç¡®ä¿æœ‰ä¸¤ä¸ªå˜åŒ–ç‚¹
        if len(change_indices) != 2:
            raise ValueError(f"æœŸæœ›2ä¸ªå˜åŒ–ç‚¹,ä½†æ‰¾åˆ°{len(change_indices)}ä¸ª")
        
        # è·å–ä¸¤ä¸ªå˜åŒ–ç‚¹
        cp1, cp2 = change_indices
        
        # åˆ†å‰²è½¨è¿¹
        stage1 = jv[:cp1]      # ç¬¬ä¸€é˜¶æ®µï¼šä»å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªå˜åŒ–ç‚¹
        stage2 = jv[cp1:cp2]   # ç¬¬äºŒé˜¶æ®µï¼šç¬¬ä¸€ä¸ªå˜åŒ–ç‚¹åˆ°ç¬¬äºŒä¸ªå˜åŒ–ç‚¹
        stage3 = jv[cp2:]      # ç¬¬ä¸‰é˜¶æ®µï¼šç¬¬äºŒä¸ªå˜åŒ–ç‚¹åˆ°ç»“æŸ
        
        return stage1,stage2,stage3
    
    def align_shoe_pose(self, shoe_model_path, real_pcd, visualize=False):
        """ç‚¹äº‘é…å‡†ä»¥ä¿®æ­£é‹å­å§¿æ€ï¼ŒåŒæ—¶å¯é€‰å¯è§†åŒ–"""
        shoe = mgm.GeometricModel(shoe_model_path)
        
        shoe_pcd = shoe.sample_surface(radius=0.001, n_samples=8000)
        shoe_pcd = shoe_pcd[shoe_pcd[:, 2] > 0.019]

        

        print("ğŸ¦¶ å¼€å§‹ICPç‚¹äº‘é…å‡†ä»¥åŒ¹é…é‹å­æ–¹å‘...")
        icp_result = registration_ptpt(shoe_pcd, real_pcd, downsampling_voxelsize=0.007)
        transformation = icp_result[2]
        shoe.homomat = transformation
        

        # æ˜¾ç¤ºé…å‡†åçš„æ¨¡å‹ç‚¹äº‘
        aligned_pcd = rm.transform_points_by_homomat(transformation, shoe_pcd.copy())
        

        print("âœ… ICPé…å‡†å®Œæˆ")
        return transformation

    def create_grasps(self):
        """ç”ŸæˆæŠ“å–å§¿æ€"""
        if os.path.exists(GRASP_PATH_SHOES):
            return
        obj = mcm.CollisionModel(SHOE_MODEL_PATH)
        grasps = gpa.plan_gripper_grasps(
            self.gripper, obj,
            angle_between_contact_normals=rm.radians(160),
            rotation_interval=rm.radians(30),
            max_samples=100,
            min_dist_between_sampled_contact_points=0.01,
            contact_offset=0.01
        )
        grasps.save_to_disk(GRASP_PATH_SHOES)
        print(f"ä¿å­˜é‹å­æŠ“å–å§¿æ€ï¼Œå…± {len(grasps)} ä¸ª")

    def execute_pick_place(self, pick_pos, place_pos, place_rot, arm : PiperArmController, robot, obstacles):
        block = mcm.CollisionModel(SHOE_MODEL_PATH)
        block.homomat = np.asarray(pick_pos, dtype=float)
        planner = ppp.PickPlacePlanner(robot)
        grasps = gg.GraspCollection.load_from_disk(GRASP_PATH_SHOES)

        mot_data = planner.gen_pick_and_place(
            obj_cmodel=block,
            end_jnt_values=robot.get_jnt_values(),
            grasp_collection=grasps,
            goal_pose_list=[(place_pos, place_rot)],
            pick_approach_direction=-rm.const.z_ax,
            place_approach_distance_list=[.05],
            place_depart_distance_list=[.05],
            pick_approach_distance=.05,
            pick_depart_distance=.05,
            obstacle_list=obstacles,
            use_rrt=False
        )
        if mot_data is None:
            print("âš ï¸ è½¨è¿¹è§„åˆ’å¤±è´¥ï¼")
            return None

        jv = mot_data.jv_list
        ev = mot_data.ev_list
        print(ev)
        binary_arr, change_indices = self.process_gripper_data(ev)
        print(change_indices)
        approach_path,pick_path,depart_path = self.split_trajectory_by_gripper(jv,change_indices)
        arm.open_gripper(width=0.08)
        arm.move_j(jv[0],speed=10,block=True)

        time.sleep(0.1)
        arm.move_jntspace_path(approach_path,speed=10)
        time.sleep(0.1)
        arm.close_gripper()
        time.sleep(0.1)
        arm.move_jntspace_path(pick_path,speed=10)
        time.sleep(0.1)
        arm.open_gripper(width = 0.08)
        time.sleep(0.1) 
        arm.move_jntspace_path(depart_path,speed=10)

        return mot_data

    def choose_arm(self, pos):
        _, y = pos
        if y[1] > -0.3:
            return self.left_arm, self.rbt_s.use_lft()
        else:
            return self.right_arm, self.rbt_s.use_rgt()

    def run(self, show_camera=False):
        obstacles = []
        self.left_arm.move_j([0] * 6, speed=20)
        self.right_arm.move_j([0] * 6, speed=20)

        objects = self.detect_shoes(show=show_camera)
        if not objects:
            print("âš ï¸ æ²¡æ£€æµ‹åˆ°é‹å­æˆ–å«å­ï¼")
            return False

        # ğŸ¦¶ é‹å­ï¼ˆè¦ICPï¼‰
        pick_obj = next((pos for cls_id, pos, _ in objects if cls_id == 0), None)
        shoe_pcd_real = next((pcd for cls_id, _, pcd in objects if cls_id == 0), None)

        shoe_pcd_real = shoe_pcd_real[
            (shoe_pcd_real[:, 2] > 0.01) & (shoe_pcd_real[:, 2] < 0.09) & (shoe_pcd_real[:, 0] < 0.6)
            & (shoe_pcd_real[:, 1] < -0.05) & (shoe_pcd_real[:, 1] > -0.6)]

        # ğŸŸ© å«å­ï¼ˆä¸ICPï¼‰
        place_obj = next((pos for cls_id, pos, _ in objects if cls_id == 1), None)

        if pick_obj is None or place_obj is None or shoe_pcd_real is None:
            print("âš ï¸ æ£€æµ‹ç»“æœä¸å®Œæ•´ï¼")
            if pick_obj is None:
                print(" -> é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°é‹å­è´¨å¿ƒ (ID 0)ã€‚")
            if shoe_pcd_real is None:
                print(" -> é”™è¯¯ï¼šæœªè·å–åˆ°é‹å­ç‚¹äº‘ (ID 0)ï¼Œæ— æ³•è¿›è¡Œ ICPã€‚")
            if place_obj is None:
                print(" -> é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°å«å­è´¨å¿ƒ (ID 1)ã€‚")
            return False

        # å¯¹é‹å­æ‰§è¡ŒICPé…å‡†
        pick_obj_pose = self.align_shoe_pose(SHOE_MODEL_PATH, shoe_pcd_real,visualize=True) 
        pick_obj_pose = pick_obj_pose.copy()
        pick_obj_pose[:3,:3] = align_z_to_up(pick_obj_pose[:3,:3])
        # æŠ“å–å¹¶æ”¾ç½®
        arm, robot = self.choose_arm((0, pick_obj))

        place_obj[2] = 0.02
        
        print(f"\n æŠ“å–é‹å­ {pick_obj} â†’ æ”¾ç½® {place_obj}")
        print(pick_obj_pose)
        place_obj_rot = rm.rotmat_from_euler(0, 0, 0)
        print(place_obj_rot)
        mot_data = self.execute_pick_place(pick_obj_pose, place_obj, place_obj_rot, arm, robot, obstacles)
        
        
        if mot_data is None:
            print("æŠ“å–å¤±è´¥")
            return False
        print("âœ… æ”¾é‹å­å®Œæˆï¼")
        return True


def main():
    task = MultiCameraShoeTask()
    try:
        task.left_arm.move_j([0] * 6, speed=20)
        task.right_arm.move_j([0] * 6, speed=20)
        success = task.run(show_camera=False)
        print("ä»»åŠ¡æˆåŠŸ âœ…" if success else "ä»»åŠ¡å¤±è´¥ âŒ")
    except KeyboardInterrupt:
        print("\nâš ï¸ æ•è·åˆ° Ctrl+C,æœºæ¢°è‡‚å›é›¶...")
        task.left_arm.move_j([0] * 6, speed=20)
        task.right_arm.move_j([0] * 6, speed=20)


if __name__ == '__main__':
    main()